{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92778525",
   "metadata": {},
   "source": [
    "# Assignment 4: Pipelines and Hyperparameter Tuning (32 total marks)\n",
    "### Due: November 22 at 11:59pm\n",
    "\n",
    "### Name: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31b39a",
   "metadata": {},
   "source": [
    "### In this assignment, you will be putting together everything you have learned so far. You will need to find your own dataset, do all the appropriate preprocessing, test different supervised learning models and evaluate the results. More details for each step can be found below.\n",
    "\n",
    "### You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf275ca7",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b67a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8219f163",
   "metadata": {},
   "source": [
    "## Step 1: Data Input (4 marks)\n",
    "\n",
    "Import the dataset you will be using. You can download the dataset onto your computer and read it in using pandas, or download it directly from the website. Answer the questions below about the dataset you selected. \n",
    "\n",
    "To find a dataset, you can use the resources listed in the notes. The dataset can be numerical, categorical, text-based or mixed. If you want help finding a particular dataset related to your interests, please email the instructor.\n",
    "\n",
    "**You cannot use a dataset that was used for a previous assignment or in class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2af8bd32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Income</th>\n",
       "      <th>ZIP.Code</th>\n",
       "      <th>Family</th>\n",
       "      <th>CCAvg</th>\n",
       "      <th>Education</th>\n",
       "      <th>Mortgage</th>\n",
       "      <th>Personal.Loan</th>\n",
       "      <th>Securities.Account</th>\n",
       "      <th>CD.Account</th>\n",
       "      <th>Online</th>\n",
       "      <th>CreditCard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.00000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2500.500000</td>\n",
       "      <td>45.338400</td>\n",
       "      <td>20.104600</td>\n",
       "      <td>73.774200</td>\n",
       "      <td>93152.503000</td>\n",
       "      <td>2.396400</td>\n",
       "      <td>1.937938</td>\n",
       "      <td>1.881000</td>\n",
       "      <td>56.498800</td>\n",
       "      <td>0.096000</td>\n",
       "      <td>0.104400</td>\n",
       "      <td>0.06040</td>\n",
       "      <td>0.596800</td>\n",
       "      <td>0.294000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1443.520003</td>\n",
       "      <td>11.463166</td>\n",
       "      <td>11.467954</td>\n",
       "      <td>46.033729</td>\n",
       "      <td>2121.852197</td>\n",
       "      <td>1.147663</td>\n",
       "      <td>1.747659</td>\n",
       "      <td>0.839869</td>\n",
       "      <td>101.713802</td>\n",
       "      <td>0.294621</td>\n",
       "      <td>0.305809</td>\n",
       "      <td>0.23825</td>\n",
       "      <td>0.490589</td>\n",
       "      <td>0.455637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9307.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1250.750000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>91911.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2500.500000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>93437.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3750.250000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>94608.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5000.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>224.000000</td>\n",
       "      <td>96651.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>635.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID          Age   Experience       Income      ZIP.Code  \\\n",
       "count  5000.000000  5000.000000  5000.000000  5000.000000   5000.000000   \n",
       "mean   2500.500000    45.338400    20.104600    73.774200  93152.503000   \n",
       "std    1443.520003    11.463166    11.467954    46.033729   2121.852197   \n",
       "min       1.000000    23.000000    -3.000000     8.000000   9307.000000   \n",
       "25%    1250.750000    35.000000    10.000000    39.000000  91911.000000   \n",
       "50%    2500.500000    45.000000    20.000000    64.000000  93437.000000   \n",
       "75%    3750.250000    55.000000    30.000000    98.000000  94608.000000   \n",
       "max    5000.000000    67.000000    43.000000   224.000000  96651.000000   \n",
       "\n",
       "            Family        CCAvg    Education     Mortgage  Personal.Loan  \\\n",
       "count  5000.000000  5000.000000  5000.000000  5000.000000    5000.000000   \n",
       "mean      2.396400     1.937938     1.881000    56.498800       0.096000   \n",
       "std       1.147663     1.747659     0.839869   101.713802       0.294621   \n",
       "min       1.000000     0.000000     1.000000     0.000000       0.000000   \n",
       "25%       1.000000     0.700000     1.000000     0.000000       0.000000   \n",
       "50%       2.000000     1.500000     2.000000     0.000000       0.000000   \n",
       "75%       3.000000     2.500000     3.000000   101.000000       0.000000   \n",
       "max       4.000000    10.000000     3.000000   635.000000       1.000000   \n",
       "\n",
       "       Securities.Account  CD.Account       Online   CreditCard  \n",
       "count         5000.000000  5000.00000  5000.000000  5000.000000  \n",
       "mean             0.104400     0.06040     0.596800     0.294000  \n",
       "std              0.305809     0.23825     0.490589     0.455637  \n",
       "min              0.000000     0.00000     0.000000     0.000000  \n",
       "25%              0.000000     0.00000     0.000000     0.000000  \n",
       "50%              0.000000     0.00000     1.000000     0.000000  \n",
       "75%              0.000000     0.00000     1.000000     1.000000  \n",
       "max              1.000000     1.00000     1.000000     1.000000  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import dataset (1 mark)\n",
    "df = pd.read_csv('./bankloan.csv')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "df.head()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20316765",
   "metadata": {},
   "source": [
    "### Questions (3 marks)\n",
    "\n",
    "1. (1 mark) What is the source of your dataset?\n",
    "1. (1 mark) Why did you pick this particular dataset?\n",
    "1. (1 mark) Was there anything challenging about finding a dataset that you wanted to use?\n",
    "\n",
    "1. The data is from Kaggle, it is a loan application dataset,  with 5000 records of loan applications, and a binary classification for approval or not for a target.  \n",
    "2. Well,  I was hunting for a set of data that I understood what the features were, and something that had lots of information in it.  This one is 5000 records, with good documentation. It additionally had features that I understood their meaning. \n",
    "3. There were some interesting ones in the medical field, however I didnt fully understand what some of the features were, so it would make things slightly more difficult to understand. Additionally some of the datasets that I had looked at also didnt have a ton of records, so I discredited those fairly quickly. I had originally tried to do a dataset of spotify data, and song popularity, but quickly found out that the corrolation there wasnt as strong as I would have liked it to be to get a reasonable model, and just made the whole process confusing.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fea4cc",
   "metadata": {},
   "source": [
    "## Step 2: Data Processing (5 marks)\n",
    "\n",
    "The next step is to process your data. Implement the following steps as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afc244d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID                    0\n",
      "Age                   0\n",
      "Experience            0\n",
      "Income                0\n",
      "ZIP.Code              0\n",
      "Family                0\n",
      "CCAvg                 0\n",
      "Education             0\n",
      "Mortgage              0\n",
      "Personal.Loan         0\n",
      "Securities.Account    0\n",
      "CD.Account            0\n",
      "Online                0\n",
      "CreditCard            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Clean data (if needed)\n",
    "df.shape\n",
    "df.dtypes\n",
    "print(df.isnull().sum()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70a8c127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age                     int64\n",
      "Experience              int64\n",
      "Income                  int64\n",
      "Family                  int64\n",
      "CCAvg                 float64\n",
      "Education               int64\n",
      "Mortgage                int64\n",
      "Personal.Loan           int64\n",
      "Securities.Account      int64\n",
      "CD.Account              int64\n",
      "Online                  int64\n",
      "CreditCard              int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Implement preprocessing steps. Remember to use ColumnTransformer if more than one preprocessing method is needed\n",
    "# Drop columns that are identifiers or way out the window of being usable for ML\n",
    "df = df.drop(columns=['ID', 'ZIP.Code'])\n",
    "\n",
    "\n",
    "# Then lets split up our data before preprocessing\n",
    "X = df.drop('Personal.Loan', axis=1)\n",
    "y = df['Personal.Loan']\n",
    "\n",
    "print(df.dtypes)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7d9c94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 12)\n"
     ]
    }
   ],
   "source": [
    "# now we have to do some further preprocessing,  since we have lots of numerical values, we should scale those, adn then we also have some categorical features that we need to use one hot for . \n",
    "\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "# Lets get all the numerical stuffs,  key and mode are left out intentionaly, as they signify categories.  key will be encoded, mode will be left as it is binary\n",
    "numerical_features = ['Experience','Income','Family','CCAvg','Education','Mortgage']\n",
    "\n",
    "categorical_features = ['CD.Account','Online','CreditCard']\n",
    "\n",
    "numerical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# All of my categorical features are already in boolean, so they do not need to be one hot encoded.\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('imputer',SimpleImputer(strategy='most_frequent'))\n",
    "])\n",
    "\n",
    "preprocessor = Pipeline([\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', numerical_pipeline, numerical_features),\n",
    "        ('cat', categorical_pipeline, categorical_features)\n",
    "    ]))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92c46b7",
   "metadata": {},
   "source": [
    "### Questions (2 marks)\n",
    "\n",
    "1. (1 mark) Were there any missing/null values in your dataset? If yes, how did you replace them and why? If no, describe how you would've replaced them and why.\n",
    "2. (1 mark) What type of data do you have? What preprocessing methods would you have to apply based on your data types?\n",
    "\n",
    "\n",
    "*ANSWER HERE*\n",
    "\n",
    "1. The data was all in good shape from the get go.  However for cases like this,  where there is lots of data already,  if there was very few records that had bad or missing data, you could just remove those records, and if there were too many of them, you could use an average or most common replacement in its place depending on the context.   \n",
    "2. I have both categorical, as well as numerical data.  So for this I scaled the numerical data, and imputed cases to mean if there was no record.  For the categorical stuff, All of the data was boolean,  so no encoding or scaling was nescessary.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a245d00",
   "metadata": {},
   "source": [
    "## Step 3: Implement Machine Learning Model (11 marks)\n",
    "\n",
    "In this section, you will implement three different supervised learning models (one linear and two non-linear) of your choice. You will use a pipeline to help you decide which model and hyperparameters work best. It is up to you to select what models to use and what hyperparameters to test. You can use the class examples for guidance. You must print out the best model parameters and results after the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5558a776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Model  Best Score  Test Score        Best Parameter  \\\n",
      "0      RandomForestClassifier    0.981891    0.979798      model__max_depth   \n",
      "1      RandomForestClassifier    0.981891    0.979798   model__n_estimators   \n",
      "2               decision_tree    0.909864    0.960784      model__max_depth   \n",
      "3                         svm    0.968000    0.989583              model__C   \n",
      "4                         svm    0.968000    0.989583         model__kernel   \n",
      "5          LogisticRegression    0.851884    0.942857              model__C   \n",
      "6          LogisticRegression    0.851884    0.942857         model__solver   \n",
      "7            LinearRegression    0.747753    0.808014  model__fit_intercept   \n",
      "8   GradientBoostingRegressor    0.975399    0.992136  model__learning_rate   \n",
      "9   GradientBoostingRegressor    0.975399    0.992136      model__max_depth   \n",
      "10  GradientBoostingRegressor    0.975399    0.992136   model__n_estimators   \n",
      "11      RandomForestRegressor    0.975317    0.992824      model__max_depth   \n",
      "12      RandomForestRegressor    0.975317    0.992824   model__n_estimators   \n",
      "\n",
      "   Parameter Value  \n",
      "0               10  \n",
      "1              100  \n",
      "2               15  \n",
      "3                1  \n",
      "4              rbf  \n",
      "5              0.1  \n",
      "6            lbfgs  \n",
      "7             True  \n",
      "8              0.1  \n",
      "9                3  \n",
      "10             100  \n",
      "11              10  \n",
      "12             100  \n"
     ]
    }
   ],
   "source": [
    "# Implement pipeline and grid search here. Can add more code blocks if necessary\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Lets just throw a ton of models at it...for good measure. \n",
    "\n",
    "model_params_reg = {\n",
    "    'LinearRegression': {\n",
    "        'model': LinearRegression(),\n",
    "        'params': {\n",
    "            'model__fit_intercept': [True, False]\n",
    "        }\n",
    "    },\n",
    "    'GradientBoostingRegressor': {\n",
    "        'model': GradientBoostingRegressor(),\n",
    "        'params': {\n",
    "            'model__n_estimators': [50, 100, 200],\n",
    "            'model__learning_rate': [0.01, 0.1, 0.2],\n",
    "            'model__max_depth': [3, 5, 7]\n",
    "        }\n",
    "    },\n",
    "    'RandomForestRegressor': {\n",
    "        'model': RandomForestRegressor(),\n",
    "        'params': {\n",
    "            'model__n_estimators': [10, 50, 100],\n",
    "            'model__max_depth': [None, 10, 20, 30]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "model_params_cls = {\n",
    "    'RandomForestClassifier': {\n",
    "        'model': RandomForestClassifier(),\n",
    "        'params': {\n",
    "            'model__n_estimators': [10, 50, 100],\n",
    "            'model__max_depth': [None, 10, 20, 30]\n",
    "        }\n",
    "    },\n",
    "    'decision_tree': {\n",
    "        'model': DecisionTreeClassifier(),\n",
    "        'params': {\n",
    "            'model__max_depth': [5, 10, 15]\n",
    "        }\n",
    "    },\n",
    "    'svm': {\n",
    "        'model': SVC(),\n",
    "        'params': {\n",
    "            'model__C': [1, 10, 100],\n",
    "            'model__kernel': ['rbf', 'linear']\n",
    "        }\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'model': LogisticRegression(),\n",
    "        'params': {\n",
    "            'model__C': [0.1, 1, 10, 100],\n",
    "            'model__solver': ['lbfgs']  \n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# make an array fer dem scores\n",
    "\n",
    "scores = []\n",
    "\n",
    "for model_name, params in model_params_cls.items():\n",
    "    pipe = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model',params['model'])\n",
    "    ])\n",
    "\n",
    "    grid = GridSearchCV(pipe, params['params'], cv=5, n_jobs=-1, scoring='precision', return_train_score=True)\n",
    "    grid.fit(X_train, y_train)\n",
    "    scores.append({\n",
    "        'model':model_name,\n",
    "        'best_score': grid.best_score_,\n",
    "        'test_score': grid.score(X_test, y_test),\n",
    "        'best_params': grid.best_params_,\n",
    "        'cv_results': grid.cv_results_\n",
    "    })\n",
    "\n",
    "for model_name, params in model_params_reg.items():\n",
    "    pipe = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model',params['model'])\n",
    "    ])\n",
    "\n",
    "    grid = GridSearchCV(pipe, params['params'], cv=5, n_jobs=-1, scoring='average_precision', return_train_score=True)\n",
    "    grid.fit(X_train, y_train)\n",
    "    scores.append({\n",
    "        'model':model_name,\n",
    "        'best_score': grid.best_score_,\n",
    "        'test_score': grid.score(X_test, y_test),\n",
    "        'best_params': grid.best_params_,\n",
    "        'cv_results': grid.cv_results_\n",
    "    })\n",
    "\n",
    "\n",
    "df_results = pd.DataFrame(columns=['Model', 'Best Score', 'Test Score', 'Best Parameter', 'Parameter Value'])\n",
    "\n",
    "rows_list = []\n",
    "\n",
    "for score in scores:\n",
    "    best_params = score['best_params']\n",
    "    for param_name, param_value in best_params.items():\n",
    "        row = {\n",
    "            'Model': score['model'],\n",
    "            'Best Score': score['best_score'],\n",
    "            'Test Score': score['test_score'],\n",
    "            'Best Parameter': param_name,\n",
    "            'Parameter Value': param_value\n",
    "        }\n",
    "        rows_list.append(row)\n",
    "df_results = pd.DataFrame(rows_list)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbd7075",
   "metadata": {},
   "source": [
    "### Questions (5 marks)\n",
    "\n",
    "1. (1 mark) Do you need regression or classification models for your dataset?\n",
    "1. (2 marks) Which models did you select for testing and why?\n",
    "1. (2 marks) Which model worked the best? Does this make sense based on the theory discussed in the course and the context of your dataset?\n",
    "\n",
    "*ANSWER HERE*\n",
    "\n",
    "I would have thought that the dataset needed classification.  There was numerous boolean features, as well as the understanding that the numerical data fro loan applications would be based on thresholds for things like income, or credit etc.  This was true, as teh Random forest classifier had the highest trainign score.  However teh randomforest generator (regression) was very close behind it, and had a higher test score.  From this, and given the relative similarity, I believe that one could make an argument for either given the shape of the data. \n",
    "\n",
    "For models,  I just threw a ton at it.  Everything I could think of hyperparameters for,  figured I'd throw it all down the hallway and check them all out.  I knew that a classification would be the best fit, and had a hunch that SVC or RandomForests would work best given the nature of the data as it better emulated the threshold based decision making of loan applications,  so easy to draw a tree for most generalizations about this. \n",
    "\n",
    "The random forest worked the best in both classification and regression, and based on the idea of how a random forest works, and the context of the data, this was completely understandable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f994e31",
   "metadata": {},
   "source": [
    "## Step 4: Validate Model (6 marks)\n",
    "\n",
    "Use the testing set to calculate the testing accuracy for the best model determined in Step 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e64c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate testing accuracy (1 mark)\n",
    "\n",
    "# See testing data in the dataframe above.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4529ba",
   "metadata": {},
   "source": [
    "\n",
    "### Questions (5 marks)\n",
    "\n",
    "1. (1 mark) Which accuracy metric did you choose? \n",
    "1. (1 mark) How do these results compare to those in part 3? Did this model generalize well?\n",
    "1. (3 marks) Based on your results and the context of your dataset, did the best model perform \"well enough\" to be used out in the real-world? Why or why not? Do you have any suggestions for how you could improve this analysis?\n",
    "\n",
    "*ANSWER HERE*\n",
    "\n",
    "Precision is the best accuracty measure for classification models where the consequences are failure are high.  So I used this value for the classification models, and the similar average precision for the regression models.  \n",
    "\n",
    "These results were very comparable to those in part 3.  In some cases the test scores were actually even higher than the training scores.  Given the shape of the data and the regemented structure of loan applications, I believe that it was reasonably easy to get this precise. \n",
    "\n",
    "This model had a 98- 99% precision in both classification and regression.  Given that the average default rate on loans written in canada last year was 1.02%, and banks underwrite for up to 2.5% generally, I think this model is good enough to be used in teh real world.  Though this model was pretty striaghtforward, with only a handful of features.  I think there is much more information that could be put in the model, as well as some more hyperparameter tuning to be done.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b238f4",
   "metadata": {},
   "source": [
    "## Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93097bfe",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*\n",
    "\n",
    "I sourced my code from kaggle.  I had orignally sourced some spotify data, however found it to be an absolute clusterf&*# to try and get any realizable results from it, with any kind of accuracy over 30%,  and further it was about 30000 records long, so it took forever to compute, and if I took a smaller dataset from there,  it lowered the accuracy, to me meaning that we were overfitting the data some.  So I went back and got this banking data which was more 'black and white' and much easier for a chump like me who was just learning.  \n",
    "\n",
    "I did the steps in teh same order as they are written here....though many times over as I had the dataset confusion.  The second time around it was much much easier.  I did go back and modify the code a bit the second time around as I had discovered that adding more models was fairly straightforward once the pipelines were written and you could as as many as you wanted without much consequence other than compute time.  \n",
    "\n",
    "I absolutely did use some generative ai.  Lots of it was more or less \"How does this corrispond to this?\"  or \"What order do these happen if I work things this way?\"  or \"How does the grid scale the testing set aside from the training data when trying to score?\"  More just learning uses.  I didnt go back and change the code after, largely just learning to write it accuratly in the first place.  \n",
    "\n",
    "The spotify data at the beginning was an absolute headache,  as I was getting 20-30% accuracies for probably a solid day and though it was an operator problem, or something wrong with my pre-processing, until I finally just figured it was the data, as music popularity is so subjective.  Once I took a more concrete set of data, these headaches went away and the learnign went up.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd97b6ac",
   "metadata": {},
   "source": [
    "## Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challenging, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "\n",
    "*ADD YOUR THOUGHTS HERE*\n",
    "\n",
    "This one was a bit challenging for sure,  as I really struggled with the hyperparameter tuning bits of it,  as they are different between all the models.  So I liked that I was forced to use and learn them in a practical application.  The 611 assignments are the best learnign aids for sure, as it is sturctured enough that you have some guidelines, but at the same time still have to do a bunch of learning and thinking.  \n",
    "\n",
    "I though this assignment was interesting, as I had to do some looking into what it takes to approve loans,  so just the context of that made it somewhat interesting for me!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241c3b12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
